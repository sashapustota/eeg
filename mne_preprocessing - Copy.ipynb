{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059acd4d",
   "metadata": {},
   "source": [
    "# EEG Preprocessing\n",
    "\n",
    "### Experiment\n",
    "In the experiment, checkerboard patterns were presented to the subject into the left and right visual field, interspersed by tones to the left or right ear. The interval between the stimuli was 750 ms. Occasionally a smiley face was presented at the center of the visual field. The subject was asked to press a key with the right index finger as soon as possible after the appearance of the face.\n",
    "\n",
    "## Loading modules & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e780bc5a-03de-4c82-8102-2cdd74b342ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mne'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rm/_7rkq2vn3718t6vzkzstldjw0000gn/T/ipykernel_8352/935323861.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmne\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mne'"
     ]
    }
   ],
   "source": [
    "# importing modules\n",
    "import os\n",
    "import numpy as np\n",
    "import mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182821f0-17ef-4412-8278-22692d3a48bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file & loading in the data\n",
    "sample_data_folder = mne.datasets.sample.data_path()\n",
    "sample_data_raw_file = os.path.join(sample_data_folder, 'MEG', 'sample',\n",
    "                                    'sample_audvis_raw.fif')\n",
    "raw = mne.io.read_raw_fif(sample_data_raw_file)\n",
    "raw.info['bads'] = []\n",
    "\n",
    "raw.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e01b4",
   "metadata": {},
   "source": [
    "Looking at the ouput from the load.data() function, try to figure out:\n",
    "\n",
    "- How many EEG channels?\n",
    "- Do you see any EEG channels marked as bad during recording?\n",
    "- What is the sampling frequency?\n",
    "- How many minutes of data were recorded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only EEG and stimulus data & plotting raw data\n",
    "raw.pick_types(meg=False, eeg=True, stim=True, exclude=[])\n",
    "raw.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e95523",
   "metadata": {},
   "source": [
    "Right now we are plotting using the default argument values of the plot() function. Try to play around with the function in order to:\n",
    "\n",
    "- Plot all EEG channels simultaneously\n",
    "- Plot a full minute of the recording\n",
    "- Give the plot a fitting title\n",
    "\n",
    "Hint: if you are in VS Code, you can see what the arguments of a function are by hovering over the function with your cursor.\n",
    "\n",
    "## Excluding bad channels\n",
    "When plotting all 60 channels, you should be able to see that one channel seems bad (i.e. it is pretty flat). We want to mark it is bad in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd249be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the name of the bad EEG channel here, e.g. ['EEG 002']\n",
    "raw.info['bads'] = []\n",
    "raw.plot(n_channels=60);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b61cc",
   "metadata": {},
   "source": [
    "Now that we have marked the channel as bad, we can exclude it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6414fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can exclude any channels in the 'bad channels' variable\n",
    "raw.pick_types(meg=False, eeg=True, stim=True, exclude='bads')\n",
    "raw.plot(n_channels=60);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd99de6",
   "metadata": {},
   "source": [
    "Now it's gone! Let's move on to filtering the data in order to enhance the signal.\n",
    "\n",
    "## Filtering\n",
    "We high-pass filter the data at 0.1 Hz and low-pass filter the data at 40 Hz, following the typical practices of EEG preprocessing. The high-pass filter minimises slow drifts in the data (e.g. scalp potentials), while the low-pass filter excludes high-frequency noise (e.g. line noise (50 Hz) or EMG (muscle-related artefacts)), with frequencies higher than the frequencies of the signal we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912b815-1b0b-4c6e-909f-a1a9597bd55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-pass filtering the data at 0.1 Hz and subsequently low-pass filtering at 40 Hz\n",
    "raw = raw.filter(0.1, None)\n",
    "raw = raw.filter(None, 40)\n",
    "\n",
    "# plotting the filtered data for inspection\n",
    "raw.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bea86b",
   "metadata": {},
   "source": [
    "Compare the filtered signal to the raw signal you plotted in the beginning. Does it look cleaner?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2402e15",
   "metadata": {},
   "source": [
    "## Artefact detection\n",
    "There are many ways to detect and deal with artefacts. Today, we simply select a value and reject anything above or below that value. We do this because we determine that values over or under this threshold are liekly not related to brain activity. For now, we just create the variable and then we use it to remove artefacts while epoching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9defc91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejecting everything over or under a threshold of 150 microvolts\n",
    "# anything above or belove this threshold is likely not brain activity but artefacts\n",
    "reject = dict(eeg=150e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa9d85",
   "metadata": {},
   "source": [
    "## Epoching\n",
    "We now want to create epochs according to the events in our data, e.g. stimuli presentation. So first, we locate the events!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68d76b-e360-45c9-b51a-27119e38960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function locates any stimulus events in the recording (e.g. presentation of stimuli or button presses)\n",
    "events = mne.find_events(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4218911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can make a dictionary of what the event IDs represent\n",
    "# by using '/' we can actually later index one dimension *across* the other, i.e. if we just write 'left' we get all events presented to the left side, both auditory and visual\n",
    "event_id = {'auditory/left': 1,\n",
    "              'auditory/right': 2,\n",
    "              'visual/left': 3,\n",
    "              'visual/right': 4,\n",
    "              'smiley': 5,\n",
    "              'button': 32\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3df9d-9a63-4f20-b33e-092dc898331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the events\n",
    "mne.viz.plot_events(events, sfreq=250, first_samp=raw.first_samp, event_id=event_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f76e4",
   "metadata": {},
   "source": [
    "\n",
    "The time window we establish span from 0.2 seconds before stimulus onset to 0.5 seconds after stimulus onset. The 200 milliseconds before the onset of the stimulus enables us to examine a baseline of activity without stimulus presentation. The 500 milliseconds after the stimulus onset denote the time in which we expect the effect to occur, since most EEG components arise before the 500 milliseconds mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced2f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establishing time window\n",
    "tmin, tmax = -0.2, 0.5\n",
    "\n",
    "# choosing only EEG channels for epoching\n",
    "picks = mne.pick_types(raw.info, meg=False, eeg=True, eog=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the epochs using the variables created in the cell above, and timelocking to the events\n",
    "# baseline time interval spans from beginning of the data (-0.2 s) to 0 s (stimulus onset)\n",
    "# we use the reject variable we created earlier in order to remove artefacts\n",
    "epochs = mne.Epochs(raw, events, event_id, tmin, tmax, picks=picks,\n",
    "                    baseline=(None, 0), reject=reject, preload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e45e7",
   "metadata": {},
   "source": [
    "As you can see, we are dropping some epochs that were deeemed 'bad' because they contain values over or under the threshold we defined, allowing us to exclude epochs with artefacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7ee93",
   "metadata": {},
   "source": [
    "## Downsampling\n",
    "Now we reduce the sample-rate to 250 Hz instead of 600. We do this after epoching, since downsampling before epoching can potentially mess with the precision of the extraction of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling to 250 Hz\n",
    "epochs_resampled = epochs.resample(250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40772b77",
   "metadata": {},
   "source": [
    "## ERPs\n",
    "Now we can group the epochs by modality (auditory/visual) and take a look at the differences between them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559befef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the '/' used in the event IDs comes in handy! To get all epochs with auditory stimuli, we can index 'auditory' across the left/right dimension (and 'visual', of course)\n",
    "# that way we don't have to specify 'auditory_left' and 'auditory_right' etc.\n",
    "aud_epochs = epochs_resampled['auditory']\n",
    "vis_epochs = epochs_resampled['visual']\n",
    "\n",
    "# plotting one channel as an example for each modality\n",
    "aud_epochs.plot_image(picks=['EEG 021']);\n",
    "vis_epochs.plot_image(picks=['EEG 021']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d382ad",
   "metadata": {},
   "source": [
    "These plots show each epoch as one row of the image map for the chosen channel, with color representing signal magnitude. The average evoked response and the sensor location are shown below the image. This gives us an idea of whether there is consistency in the signal.\n",
    "\n",
    "In your view, which is the most consistent signal of the two? Why do you think that is?\n",
    "\n",
    "(hint 1: where is the EEG 021 channel located on the scalp?)\n",
    "\n",
    "(hint 2: maybe try using plot_sensors and flagging show_names=True)\n",
    "\n",
    "Which channel would you choose if you were to reverse the pattern of the signal consistencies? Go ahead and try it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e020fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating evokeds for auditory condition by averaging over epochs\n",
    "aud_evoked = aud_epochs.average()\n",
    "\n",
    "# creating evokeds for visual condition\n",
    "vis_evoked = vis_epochs.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the two evokeds together\n",
    "mne.viz.plot_compare_evokeds(dict(auditory=aud_evoked, visual=vis_evoked),\n",
    "                             legend='upper left', show_sensors='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c0b5e0",
   "metadata": {},
   "source": [
    "Looking a bit closer at the averaged responses, here visualized with the gfp (global field power).\n",
    "\n",
    "GFP is actually the same as taking the standard deviation across all channels in each time sample. This tells us in which timepoints we see the biggest difference between different channels, which in turn is indicative of some kind of (more or less) focal activity. Quite neat actually.\n",
    "\n",
    "Looking at the plot, where in time do you see the strongest response in the auditory and visual conditions, respectively?\n",
    "\n",
    "Do those timepoints match any well-known components relevant in this context? \n",
    "\n",
    "(hint: N100 and N170)\n",
    "\n",
    "Now compare with the \"joint\" plots below - do those plots match your observations from the GFP-plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65170e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_evoked.plot_joint(picks='eeg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ea4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_evoked.plot_joint(picks='eeg');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e450aa3",
   "metadata": {},
   "source": [
    "As a final note, we can create contrasts (aka. difference waves) on the fly by using the combine_evoked-function (in combination with the weights=[1, -1]-parameter).\n",
    "\n",
    "This also gives us a chance to try out one final plotting function, namely the plot_topo-function. This function plots the difference waves (or whichever traces we choose to plot) for each channel in their positions on the scalp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e395d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_diff = mne.combine_evoked([aud_evoked, vis_evoked], weights=[1, -1])\n",
    "evoked_diff.pick_types(eeg=True).plot_topo(color='r', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f808901f",
   "metadata": {},
   "source": [
    "# Optional/bonus\n",
    "## Independent component analysis (ICA)\n",
    "ICA can be used for artefact detection, since it identifies seperate components of the signal that have been combined during recording. That means that we can actually separate noise compoenents, such as eye blinks, from the rest of the signal, and thereby exclude them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a01b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up and fit the ica with 800 iterations with a random seed at 97\n",
    "# n_components=0.95 ensures that the number of components selected explain at least 95% of the variance in the data\n",
    "ica = mne.preprocessing.ICA(n_components=0.95, random_state=97, max_iter=800)\n",
    "ica.fit(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96823b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the ica components\n",
    "ica.plot_components();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the time series of the ica\n",
    "ica.plot_sources(raw, show_scrollbars=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3488dda8",
   "metadata": {},
   "source": [
    "From visual inspection of the topographic maps and the time series plots of the components, we can see that the first component seems to capture noise. Consequently, we can remove it and thereby extract these artefacts from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f32ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first component is excluded based on visual inspection\n",
    "ica.exclude = [0]\n",
    "ica.plot_properties(raw, picks=ica.exclude);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ffd236",
   "metadata": {},
   "source": [
    "We can now apply the ICA to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55147cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the ica to the data\n",
    "ica.apply(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8289d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the data after filtering and ica\n",
    "raw.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1842cc3a",
   "metadata": {},
   "source": [
    "Now try to run \n",
    "- artefact-detection\n",
    "- epoching\n",
    "- downsampling\n",
    "- the single-trial plots\n",
    "- the evokeds plots\n",
    "\n",
    "(i.e. exactly like you just did in the above)\n",
    "\n",
    "Now compare your results with and without ICA - can you tell any differences between the two, just qualitatively speaking?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad5c30fea5466ca3b04feb7e0a99857a92ee68be4c3f4107148abce45ea17a05"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
